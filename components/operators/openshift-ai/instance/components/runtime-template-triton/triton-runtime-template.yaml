kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: trition-serving-runtime
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["multi"]'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: triton-24.07
      labels:
        name: triton-24.07
      annotations:
        maxLoadingConcurrency: '2'
        openshift.io/display-name: Triton runtime 24.07
        enable-auth: 'false'
        enable-route: 'true'
        opendatahub.io/template-display-name: Triton runtime 24.07
        opendatahub.io/template-name: triton-24.07
        argocd.argoproj.io/sync-wave: "1"
        argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    spec:
      annotations:
        prometheus.io/port: '8002'
        prometheus.io/path: "/metrics"
        serving.knative.openshift.io/enablePassthrough: "true"
        prometheus.io/scrape: 'true'
        sidecar.istio.io/inject: "true"
        sidecar.istio.io/rewriteAppHTTPProbers: "true"
      supportedModelFormats:
        - name: keras
          version: "2"
          autoSelect: true
        - name: onnx
          version: "1"
          autoSelect: true
        - name: pytorch
          version: "1"
          autoSelect: true
        - name: tensorflow
          version: "1"
          autoSelect: true
        - name: python
          version: "1"
          autoSelect: true
        - name: tensorrt
          version: "7"
          autoSelect: true
        - name: bls
          version: "1"
          autoSelect: true
        - name: ensemble
          version: "1"
          autoSelect: true
        - name: fil
          version: "1"
          autoSelect: true
      protocolVersions:
        - grpc-v2
        - v2
      multiModel: true
      grpcEndpoint: 'port:8085'
      grpcDataEndpoint: 'port:8001'
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 2Gi
        - name: local-model-repo
          emptyDir: {}
      containers:
        - name: triton
          image: 'nvcr.io/nvidia/tritonserver:24.07-py3'
          command:
            - tritonserver
          args:
            - --model-repository=/mnt/models/
#            - --model-control-mode=implicit
            - --strict-readiness=false
            - --allow-http=true
            - --allow-sagemaker=false
#            - --load-model=fraud
            - --allow-metrics=false
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: local-model-repo
              mountPath: /mnt/models/
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: '5'
              memory: 1Gi
          livenessProbe:
            exec:
              command:
                - curl
                - '--fail'
                - '--silent'
                - '--show-error'
                - '--max-time'
                - '9'
                - 'http://localhost:8000/v2/health/live'
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 10
      builtInAdapter:
        serverType: triton
        runtimeManagementPort: 8001
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000